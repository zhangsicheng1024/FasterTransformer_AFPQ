## Done

- 跑通MoE

- 看看T5-base的GEMM的性能表现 

  加速效果大概x2
  
  ```
  HuggingFace: T5-base
  FasterTransformer: switch-base-8 
  python ../examples/pytorch/t5/perf_benchmark.py \
          --batch_size 1 \
          --beam_width 4 \
          --seq_len 256 \
          --data_type fp32 \
          --test_time 0123 \
          --sampling_topk 1 \
          --model_type Megatron-DeepSpeed \
          --ckpt_path /root/FasterTransformer/fake_t5_moe_ckpt \
          --model t5-base
          
  [INFO] hf-beamsearch translates 10 batches taking 25.98 sec to translate 2560 tokens (2598.2951 ms per batch), 99 tokens/sec.
  [INFO] ft-beamsearch translates 10 batches taking 14.53 sec to translate 2560 tokens (1452.6346 ms per batch), 176 tokens/sec.
  [INFO] hf-sampling translates 10 batches taking 22.78 sec to translate 2560 tokens (2277.5482 ms per batch), 112 tokens/sec.
  [INFO] ft-sampling translates 10 batches taking 12.93 sec to translate 2560 tokens (1292.7037 ms per batch), 198 tokens/sec.
  ```
  
- 读Encoder MoE layer的具体实现
  
## Doing

- 读Decoder MoE layer的具体实现

## To do

- 实现Encoder的fetch on demand

- 实现Decoder的prefetch

  

