#### 本周工作

- 读了两三篇MoE相关的论文，入门CUDA

- skim了一下T5 Encoder的内存使用方式



#### T5 Encoder GPU memory

- forward前：`input_tensors`, `t5_encoder_weights` , `t5_encoder_weights`，都在GPU分配好

- forward时： 中间变量 `allocateBuffer` ，包括一些子layer的forward中也会有



#### 实现Offload的基本思路

1. 改allocator，所有buffer分配时，都放在host mem。相当于GPU memory是cold的
2. 每次启动kernel function前把需要用到的buffer cache到GPU
3. 这里要实现evict和pin：需要用的buffer要pin住
4. evict算法？



#### 一些问题

1. 需要找一个Task给我修改过的代码来跑，检验实现是否正确
2. 对AI模型结构不熟悉，参数和机制都很多细节不太清楚
3. CUDA内存模型有复杂的调优细节 ---> 打算先按简单的gpu-cpu模型来弄一个能跑起来的再说
4. 实现预先gate的时候需要基于事件的同步机制
5. 可以假设GPU上的memory address和CPU上不同吗

