## Done

- Hugging face T5 / Faster Transformer T5

- 用Docker配了下Faster Transformer环境

- 跑了一下T5-small，FT在3050上也能取得11倍左右的加速效果，与其present的加速效果吻合

    ```bash
    [INFO] hf-beamsearch translates 3 batches taking 84.58 sec to translate 768 tokens (28193.6787 ms per batch), 9 tokens/sec
    [INFO] ft-beamsearch translates 3 batches taking 7.77 sec to translate 768 tokens (2590.8383 ms per batch), 99 tokens/sec.
    ```

## Need to discuss

- 默认参数下Warmup Iteration 要跑十分钟，不知道是哪里的问题；虚拟机的IO？先不着急解决；机器下来了再说

  ```
  python ../examples/pytorch/t5/perf_benchmark.py \
          --batch_size 1 \
          --beam_width 4 \
          --seq_len 256 \
          --data_type fp16 \
          --test_time 01 \
          --sampling_topk 1 \
          --model t5-small \
          --iteration 2
  ```

- 本地运行经常内存不够，很抓狂，需要尽快给我机器；学生能申请的集群没有GPU节点

- 主要需要关注的函数是
  ```C++
  moe_kernel.cu: void CutlassMoeFCRunner<T, WeightType, Enable>::run_moe_fc(...)
  ```

  包含：initialize routing+FC+activation+FC
  
- Calling path

  ```C++
  run_moe_fc
  moe_gemm_bias_act (不同激活函数)
  run_gemm（尝试不同的GEMM配置，选择最好的）
  dispatch_to_arch（不同架构）
  dispatch_moe_gemm_to_cutlass
  dispatch_gemm_config
  dispatch_stages
  (3rdparty...)
  ```

  路径上非常多泛型分化：不同架构，不同Config

- 目前的MoE实现，大体看起来是这样的：

  - 一开始就将所有的expert的参数放在GPU里，一个指针传到底
  - token的重排
  - .....?
  
  
  
  ```c++
  run_moe_fc(...) {
  	topk_gating_softmax_kernelLauncher
  	initialize_moe_routing_kernelLauncher
  	compute_total_rows_before_expert
  	moe_gemm_bias_act
  	moe_gemm
  }
  ```
  怎么改，暂时没有头绪，需要再仔细看代码，不好懂；挺复杂，需要边run边看
  
  主要是这几个函数的input output要搞清楚什么含义，才能下结论
