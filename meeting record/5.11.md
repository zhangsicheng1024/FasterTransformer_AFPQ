

# Done

| Frameworks-models-scale (remarks) | Performance (token/s)         | Memory |
| --------------------------------- | ----------------------------- | ------ |
| FT-T5-base-8 (GPU only)           | 170                           |        |
| FT-T5-base-8 (fetch on demand)    | 87                            |        |
| FT-T5-base-8 (prefetch)           | 119 (=1.26 * 87)              |        |
|                                   |                               |        |
| FT-T5-large-128 (GPU only)        | Out of GPU Memory (need 120G) |        |
| FT-T5-large-128 (fetch on demand) | 29                            |        |
| FT-T5-large-128 (prefetch)        | 44 (=1.51*29)                 |        |

Setting: A6000, batch = 1, beam = 4.

结论：

1. 和纯GPU相比，offoading总是慢的。慢多少？取决于模型大小。
   1. base-8，只有纯GPU的50%。
   2. large-128，测不出来
2. prefetch的优化效果
   1. base-8，在offload的基础上，prefetch将吞吐提高29%。
   2. large-128，在offload的基础上，prefetch将吞吐提高50%。
   3. 小模型为什么提升小，猜测是因为prefetch本身的开销占比相对大了。

3. 更大的模型，暂时测不了，Host Memory只有256G
4. 模型太大，各种奇怪的地方会出问题，初始化weights之类的预处理，占运行时间的大部分（虽然没有测量进去，但是影响做实验效率）



# TODO

- 测不同的batch
- large-128的bottleneck？
- memory usage，算一下
- 看一下fetch fp16的话，会不会有提升
- FT的多gpu支持

