## Done

- 在batch_size=1下的性能表现：

  |            |                        |      |
  | ---------- | ---------------------- | ---- |
  | HF-T5-base | FT-T5-base             | 4.3x |
  | HF-T5-base | FT-T5-MoE(未实现fetch) | 1.8x |
  |            |                        |      |
  
  ```
   
  python ../examples/pytorch/t5/perf_benchmark.py \
          --batch_size 1 \
          --beam_width 4 \
          --seq_len 256 \
          --data_type fp32 \
          --test_time 0123 \
          --sampling_topk 1 \
          --model_type Megatron-DeepSpeed \
          --ckpt_path /root/FasterTransformer/fake_t5_moe_ckpt \
          --model t5-base
          
  [INFO] hf-beamsearch translates 10 batches taking 25.68 sec to translate 2560 tokens (2568.4951 ms per batch), 100 tokens/sec.
  [INFO] ft-beamsearch translates 10 batches taking 14.53 sec to translate 2560 tokens (1453.2931 ms per batch), 176 tokens/sec.
  [INFO] hf-sampling translates 10 batches taking 22.20 sec to translate 2560 tokens (2220.4317 ms per batch), 115 tokens/sec.
  [INFO] ft-sampling translates 10 batches taking 12.94 sec to translate 2560 tokens (1293.8525 ms per batch), 198 tokens/sec.
  
  
  
  [INFO] hf-beamsearch translates 10 batches taking 25.40 sec to translate 2560 tokens (2540.1537 ms per batch), 101 tokens/sec.
  [INFO] ft-beamsearch translates 10 batches taking 5.93 sec to translate 2560 tokens (593.2473 ms per batch), 432 tokens/sec.
  [INFO] hf-sampling translates 10 batches taking 21.92 sec to translate 2560 tokens (2192.2260 ms per batch), 117 tokens/sec.
  [INFO] ft-sampling translates 10 batches taking 4.30 sec to translate 2560 tokens (429.9716 ms per batch), 595 tokens/sec.
  ```
## Doing

- 实现decoder的prefetch（写完了，正在调）

## Todo

- 实现encoder的fetch

