

# Done

- 用event同步：尝试了一下在现有框架里加，发现不行，改动太大，可能要重构很多东西。
- large-128的profile：试了，跑了一个小时跑不出来，比较奇怪。
- 计算了一下memory开销（不计入中间的临时buffer）

| Frameworks-models-scale (remarks) | Performance (token/s) | model size (fp32) | GPU Memory (fp32) |
| --------------------------------- | --------------------- | ----------------- | ----------------- |
| FT-T5-base-8 (GPU only)           | 170                   | 2.1GB             | 2.1GB             |
| FT-T5-base-8 (fetch on demand)    | 87                    | 2.1GB             | 0.65GB            |
| FT-T5-base-8 (prefetch)           | 119 (=1.26 * 87)      | 2.1GB             | 0.65GB            |
|                                   |                       |                   |                   |
| FT-T5-large-128 (GPU only)        | Out of GPU Memory     | 97.5 GB           | 97.5 GB           |
| FT-T5-large-128 (fetch on demand) | 29                    | 97.5 GB           | 2.3GB             |
| FT-T5-large-128 (prefetch)        | 44 (=1.51*29)         | 97.5 GB           | 2.3GB             |













large-128：

一个非moe层：

- attention: 1024 x 1024 x 4 = 4194304
- ff:  1024 x 4096 x 2 = 8388608
- 总共  12582912

一个moe层：

- attention: 1024 x 1024 x 4 = 4194304
- ff:  1024 x 4096 x 2 x 128 = 1073741824
- 总共  1077936128

总共：48层，24层moe，24层非moe

(12582912+ 1077936128) * 24 = 97.5 GB





base-8：

一个非moe层：

- attention: 768 x 768 x 4 = 2359296
- ff:  768 x 3072 x 2 = 4718592
- 总共  7077888

一个moe层：

- attention: 768 x 768 x 4 = 2359296
- ff:  768 x 3072 x 2 x 8 = 37748736
- 总共  40108032

总共：24层，12层moe，12层非moe

(7077888 + 40108032) * 12 = 2.1 GB





# TODO

- 测不同的batch
- large-128的bottleneck？
- memory usage，算一下
- 看一下fetch fp16的话，会不会有提升
- FT的多gpu支持

