## Done

- 把不需要的部分从编译里摘出去
- 跑通MoE

## Doing

- 读MoE layer的具体实现

## To do

- 想一下如何实现提前gate的内存管理

- Discussion: MoE 模型的加速效果比较一般；显卡利用率有很大的提升，感觉不是计算bound

  ```
  (base) ➜  build git:(main) python ../examples/pytorch/t5/perf_benchmark.py \
          --batch_size 1 \
          --beam_width 4 \
          --seq_len 256 \
          --data_type fp32 \
          --test_time 0123 \
          --sampling_topk 1 \
          --model_type Megatron-DeepSpeed \
          --ckpt_path /root/FasterTransformer/fake_t5_moe_ckpt \
  
  [INFO] hf-beamsearch translates 10 batches taking 14.24 sec to translate 2560 tokens (1423.6947 ms per batch), 180 tokens/sec.
  [INFO] ft-beamsearch translates 10 batches taking 11.31 sec to translate 2560 tokens (1130.9489 ms per batch), 226 tokens/sec.
  [INFO] hf-sampling translates 10 batches taking 12.41 sec to translate 2560 tokens (1240.7517 ms per batch), 206 tokens/sec.
  [INFO] ft-sampling translates 10 batches taking 10.12 sec to translate 2560 tokens (1011.9518 ms per batch), 253 tokens/sec.
  ```

  