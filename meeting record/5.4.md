## Done

- performance:

  |                                                 |      | Performance token/s |
  | ----------------------------------------------- | ---- | ------------------- |
  | HF-T5-base                                      |      | 1x                  |
  | FT-T5-base                                      |      | 4.3x                |
  | FT-T5-MoE(w/o prefetch and offloading)          |      | 1.8x                |
  | FT-T5-MoE(with decoder prefetch and offloading) |      | 0.8x                |
  
  ```
   
  python ../examples/pytorch/t5/perf_benchmark.py \
          --batch_size 1 \
          --beam_width 4 \
          --seq_len 256 \
          --data_type fp32 \
          --test_time 0123 \
          --sampling_topk 1 \
          --model_type Megatron-DeepSpeed \
          --ckpt_path /root/FasterTransformer/fake_t5_moe_ckpt \
          --model t5-base
          
  [INFO] hf-beamsearch translates 10 batches taking 25.68 sec to translate 2560 tokens (2568.4951 ms per batch), 100 tokens/sec.
  [INFO] ft-beamsearch translates 10 batches taking 14.53 sec to translate 2560 tokens (1453.2931 ms per batch), 176 tokens/sec.
  [INFO] hf-sampling translates 10 batches taking 22.20 sec to translate 2560 tokens (2220.4317 ms per batch), 115 tokens/sec.
  [INFO] ft-sampling translates 10 batches taking 12.94 sec to translate 2560 tokens (1293.8525 ms per batch), 198 tokens/sec.
  
  
  
  [INFO] hf-beamsearch translates 10 batches taking 25.40 sec to translate 2560 tokens (2540.1537 ms per batch), 101 tokens/sec.
  [INFO] ft-beamsearch translates 10 batches taking 5.93 sec to translate 2560 tokens (593.2473 ms per batch), 432 tokens/sec.
  [INFO] hf-sampling translates 10 batches taking 21.92 sec to translate 2560 tokens (2192.2260 ms per batch), 117 tokens/sec.
  [INFO] ft-sampling translates 10 batches taking 4.30 sec to translate 2560 tokens (429.9716 ms per batch), 595 tokens/sec.
  ```
## Doing

-  Profiling

## Todo

- Encoder prefetch

